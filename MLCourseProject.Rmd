---
title: "ML-Project"
author: "Pratik Verma"
date: "December 24, 2015"
output:
  html_document:
    theme: cerulean
---

### Intro:

###  Data Load
```{r}
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
library(dplyr)
library(reshape2)


 set.seed(1234)
 x <<- read.csv("pml-training.csv")
 mlFinalTest <<- read.csv("pml-testing.csv")
```


### Step 1 Cleaning Data:

In this process elimination of features is done which may not be that relevant for learning algorithm. They might have the absence of values beyond a certain threshold.
There are lot of columns in the training set having more than 97% of values as NA/null. Also it has columns like "user_name" and timestamps which do not provide relevant information. This step also helps us reduce dimensions of feature vector.

### Step 2: Correlation/Relation:
In this step tries to find out highly correlated feature pairs. Threshold > 80% correlation

        Variabl 1             Variable 2     Correlation

      total_accel_belt            roll_belt 0.9809241
       gyros_forearm_z     gyros_dumbbell_z 0.9330422
          accel_belt_y     total_accel_belt 0.9278069
          accel_belt_y            roll_belt 0.9248983
         magnet_belt_x         accel_belt_x 0.8920913
      accel_dumbbell_z         yaw_dumbbell 0.8491322
       gyros_forearm_z      gyros_forearm_y 0.8455626
              yaw_belt            roll_belt 0.8152297
          magnet_arm_z         magnet_arm_y 0.8144455
          magnet_arm_x          accel_arm_x 0.8142732
      accel_dumbbell_x       pitch_dumbbell 0.8082885

This is pretty good indicator that preprocessing with PCA should be used here or any column can be dropped from above pair.

```{r}
 y <- names(x)
 filterData <<- data.frame()
 cols <<- numeric()
 
 #based on correlation between columns following columns should not be considered for training
 ####Correlation Code ######
 #### d_cor <<- as.matrix(cor(filterData[,1:44]))
 #### d_cor_melt <<- arrange(melt(d_cor), -abs(value))
 #### highCors <<- subset(d_cor_melt, value > .5)
 ###########################
 ignoreColumns <- c("roll_belt","gyros_dumbbell_z",
                 "magnet_belt_x","accel_belt_y","accel_dumbbell_z","gyros_forearm_y",
                 "magnet_arm_y","accel_arm_x","accel_dumbbell_x")
 
 ###### Also it has been noticed that lot of columns have values empty or NA
 ### Column 1:6 has been ignored because they dont provide any relevant information
 j <- 1
 for(i in y) {
     s<- paste(i,sum(is.na(x[[i]])),sum(x[[i]] == ""), sum(is.null(x[[i]])), sep="  ")
     if(sum(is.na(x[[i]]))!=19216 && sum(x[[i]] == "")!=19216 && sum(is.null(x[[i]]))!=19216 && (j>6)) {
         if(!(i %in% ignoreColumns)) {
             cols <<- append(cols, j)
         }
     }
     j <- j+1
 }
# Based on above Facts, filtering the training data 
 filterData <<- subset(x[,cols])
 
 # This analogy is applied on actual test data
 testFilterData <<- subset(mlFinalTest[,cols])

```


### Cross-validation

K-Fold cross validation has been chosen here where K = 10 and repeats =1 in trainControl method. For preprocessing with PCA threshold was set to 90%.
Training data was partitioned with p = 0.70 So, Hence training data was 30% of total data. So,

trainingData is 70% of actual training set,
testingData is 30% of actual training set

Data Partition:

```{r}
 inTrain <- createDataPartition(y=filterData$classe, p=.7, list=FALSE)
 trainingData <<- filterData[inTrain,]
 testingData <<- filterData[-inTrain,]
```


```{r}
 ### repeats is set to 1
 ctrl <- trainControl(method = "repeatedcv", repeats = 1)
 ### preprocessing with PCA 
 preObj <- preProcess(trainingData[,-45], method=c("center", "scale", "pca"), thresh=0.9)
 trainPC <- predict(preObj, trainingData[,-45])
 ## training randomforest model
 modelFit <<- train(trainingData$classe ~ ., data=trainPC, method="rf", trControl=ctrl)
```


### Result and Conclusion:

modelFit$finalModel

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 2.37%
Confusion matrix:
     A    B    C    D    E class.error
A 3870   11   15    6    4  0.00921659
B   46 2566   34    6    6  0.03461249
C    6   40 2332   16    2  0.02671119
D    5    6   80 2156    5  0.04262877
E    0    9   13   15 2488  0.01465347

### Prediction on test data

```{r}
 testPC <- predict(preObj, testingData[,-45])
 predictions <- predict(modelFit, newdata=testPC)
 confusionMatrix(predictions, testingData$classe)
```


### Predictions on final Test set

```{r}
 finaltestPC <- predict(preObj, testFilterData[,-45])
 predictions <- predict(modelFit, newdata=finaltestPC)
 predictions

```

